{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3da7357",
   "metadata": {},
   "source": [
    "# Amazon Keyword Insights â€” Analysis Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3df2e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load & Clean the Amazon Cerebro dataset ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os\n",
    "\n",
    "# Auto-pick a CSV inside data/\n",
    "candidates = sorted(glob.glob(os.path.join(\"data\", \"*.csv\")))\n",
    "file_path = candidates[0] if candidates else r\"data\\YOUR_FILE.csv\"\n",
    "\n",
    "print(\"Using file:\", file_path)\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Basic cleanup\n",
    "df = df.loc[:, ~df.columns.str.contains(r\"^Unnamed\", na=False)]\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Drop fully empty rows\n",
    "df.dropna(how=\"all\", inplace=True)\n",
    "\n",
    "# Standardize column names to snake_case\n",
    "def to_snake(s: str) -> str:\n",
    "    s = s.strip().replace(\"%\", \"pct\")\n",
    "    s = s.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    s = s.replace(\".\", \"\").replace(\"/\", \"_\")\n",
    "    s = s.replace(\"-\", \" \")\n",
    "    s = \"_\".join(s.split())\n",
    "    return s.lower()\n",
    "\n",
    "df.rename(columns={c: to_snake(c) for c in df.columns}, inplace=True)\n",
    "\n",
    "# Coerce numeric where possible\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == \"object\":\n",
    "        coerced = pd.to_numeric(df[col].astype(str).str.replace(\",\", \"\"), errors=\"coerce\")\n",
    "        if coerced.notna().mean() >= 0.8:\n",
    "            df[col] = coerced\n",
    "\n",
    "# Handle missing values\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "obj_cols = df.select_dtypes(exclude=[np.number]).columns\n",
    "df[num_cols] = df[num_cols].apply(lambda s: s.fillna(s.median()))\n",
    "df[obj_cols] = df[obj_cols].fillna(\"Unknown\")\n",
    "\n",
    "# Remove duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Parse date-like columns\n",
    "for col in df.columns:\n",
    "    if \"date\" in col:\n",
    "        df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"âœ… Cleaned shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c828d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Engineering & Global Best/Top5 Keywords ---\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _as_fraction(series: pd.Series) -> pd.Series:\n",
    "    if series.empty:\n",
    "        return series\n",
    "    m = series.max(skipna=True)\n",
    "    if pd.isna(m):\n",
    "        return series\n",
    "    if m <= 1.0:\n",
    "        return series.clip(lower=0, upper=1)\n",
    "    if m <= 100.0:\n",
    "        return (series / 100.0).clip(lower=0, upper=1)\n",
    "    return series\n",
    "\n",
    "def safe_div(a, b):\n",
    "    return np.where(b == 0, np.nan, a / b)\n",
    "\n",
    "col_map = {\n",
    "    \"keyword\": \"keyword_phrase\" if \"keyword_phrase\" in df.columns else None,\n",
    "    \"sv\": \"search_volume\" if \"search_volume\" in df.columns else None,\n",
    "    \"sales\": \"keyword_sales\" if \"keyword_sales\" in df.columns else None,\n",
    "    \"click_share\": \"aba_total_click_share\" if \"aba_total_click_share\" in df.columns else None,\n",
    "    \"conv_share\": \"aba_total_conv_share\" if \"aba_total_conv_share\" in df.columns else None,\n",
    "    \"sponsored_asins\": \"sponsored_asins\" if \"sponsored_asins\" in df.columns else None,\n",
    "    \"competing_products\": \"competing_products\" if \"competing_products\" in df.columns else None,\n",
    "    \"title_density\": \"title_density\" if \"title_density\" in df.columns else None,\n",
    "    \"iq\": \"cerebro_iq_score\" if \"cerebro_iq_score\" in df.columns else None,\n",
    "}\n",
    "\n",
    "df[\"ctr_est\"] = _as_fraction(df[col_map[\"click_share\"]]) if col_map[\"click_share\"] else np.nan\n",
    "df[\"cvr_est\"] = _as_fraction(df[col_map[\"conv_share\"]]) if col_map[\"conv_share\"] else np.nan\n",
    "\n",
    "if col_map[\"sv\"]:\n",
    "    df[\"est_clicks\"] = df[\"ctr_est\"] * df[col_map[\"sv\"]]\n",
    "    df[\"est_conversions\"] = df[\"cvr_est\"] * df[\"est_clicks\"]\n",
    "else:\n",
    "    df[\"est_clicks\"] = np.nan\n",
    "    df[\"est_conversions\"] = np.nan\n",
    "\n",
    "if col_map[\"sv\"] and col_map[\"sales\"]:\n",
    "    df[\"sales_per_search\"] = safe_div(df[col_map[\"sales\"]], df[col_map[\"sv\"]])\n",
    "else:\n",
    "    df[\"sales_per_search\"] = np.nan\n",
    "\n",
    "if col_map[\"sv\"] and col_map[\"competing_products\"]:\n",
    "    df[\"comp_per_1k_sv\"] = safe_div(df[col_map[\"competing_products\"]], (df[col_map[\"sv\"]]/1000.0))\n",
    "else:\n",
    "    df[\"comp_per_1k_sv\"] = np.nan\n",
    "\n",
    "if col_map[\"sponsored_asins\"]:\n",
    "    max_sponsored = (df[col_map[\"sponsored_asins\"]].max() or 1)\n",
    "    df[\"sponsor_intensity\"] = df[col_map[\"sponsored_asins\"]] / max_sponsored\n",
    "else:\n",
    "    df[\"sponsor_intensity\"] = np.nan\n",
    "\n",
    "if col_map[\"sv\"]:\n",
    "    denom = (1.0 + df[\"comp_per_1k_sv\"].fillna(df[\"comp_per_1k_sv\"].median()))\n",
    "    sponsor_term = (1.0 + df[\"sponsor_intensity\"].fillna(df[\"sponsor_intensity\"].median()))\n",
    "    df[\"opportunity_index\"] = safe_div(df[col_map[\"sv\"]], denom * sponsor_term)\n",
    "else:\n",
    "    df[\"opportunity_index\"] = np.nan\n",
    "\n",
    "if col_map[\"title_density\"]:\n",
    "    df[\"opportunity_adj_title\"] = safe_div(df[\"opportunity_index\"], (1 + df[col_map[\"title_density\"]]))\n",
    "else:\n",
    "    df[\"opportunity_adj_title\"] = np.nan\n",
    "\n",
    "if col_map[\"iq\"] and col_map[\"sv\"]:\n",
    "    df[\"iq_per_1k_sv\"] = safe_div(df[col_map[\"iq\"]], (df[col_map[\"sv\"]] / 1000.0))\n",
    "else:\n",
    "    df[\"iq_per_1k_sv\"] = np.nan\n",
    "\n",
    "# Save cleaned dataset\n",
    "out_path = file_path.rsplit(\".\", 1)[0] + \"_CLEANED.csv\"\n",
    "df.to_csv(out_path, index=False)\n",
    "print(\"ðŸ’¾ Saved cleaned dataset to:\", out_path)\n",
    "\n",
    "# Global best/top5 keywords\n",
    "kw_col = \"keyword_phrase\" if \"keyword_phrase\" in df.columns else None\n",
    "sv_col = \"search_volume\" if \"search_volume\" in df.columns else None\n",
    "sales_col = \"keyword_sales\" if \"keyword_sales\" in df.columns else None\n",
    "\n",
    "if all(c in df.columns for c in [kw_col, sv_col, sales_col]):\n",
    "    kw_df = df.dropna(subset=[sv_col, sales_col])\n",
    "    kw_df = kw_df[kw_df[sv_col] > 0]\n",
    "    kw_df[\"sales_per_search\"] = kw_df[sales_col] / kw_df[sv_col]\n",
    "    kw_df[\"score\"] = kw_df[sv_col] * kw_df[\"sales_per_search\"]\n",
    "    best_keyword = kw_df.loc[kw_df[\"score\"].idxmax(), [kw_col, sv_col, sales_col, \"sales_per_search\", \"score\"]]\n",
    "    top5_keywords = kw_df.sort_values(\"score\", ascending=False).head(5)\n",
    "    print(\"âœ… Best keyword overall:\")\n",
    "    display(best_keyword.to_frame().T)\n",
    "    print(\"\\nâœ… Top 5 keywords overall:\")\n",
    "    display(top5_keywords)\n",
    "else:\n",
    "    print(\"Keyword-level columns not found; skipping global best/top5.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd8ffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ASIN-based mapping & plots ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "kw_col = \"keyword_phrase\" if \"keyword_phrase\" in df.columns else None\n",
    "sv_col = \"search_volume\" if \"search_volume\" in df.columns else None\n",
    "sales_col = \"keyword_sales\" if \"keyword_sales\" in df.columns else None\n",
    "\n",
    "asin_cols = [c for c in df.columns if c.startswith(\"b0\")]  # snake_cased ASIN columns\n",
    "\n",
    "if kw_col and sv_col and sales_col and asin_cols:\n",
    "    long_df = df.melt(\n",
    "        id_vars=[kw_col, sv_col, sales_col],\n",
    "        value_vars=asin_cols,\n",
    "        var_name=\"asin\",\n",
    "        value_name=\"rank\"\n",
    "    )\n",
    "    long_df = long_df.dropna(subset=[\"rank\", sv_col, sales_col])\n",
    "    long_df = long_df[long_df[sv_col] > 0]\n",
    "    long_df[\"rank\"] = pd.to_numeric(long_df[\"rank\"], errors=\"coerce\")\n",
    "    long_df = long_df.dropna(subset=[\"rank\"])\n",
    "\n",
    "    long_df[\"sales_per_search\"] = long_df[sales_col] / long_df[sv_col]\n",
    "    long_df[\"rank_weight\"] = 1 / (1 + long_df[\"rank\"])\n",
    "    long_df[\"score\"] = long_df[sv_col] * long_df[\"sales_per_search\"] * long_df[\"rank_weight\"]\n",
    "\n",
    "    best_per_asin = long_df.loc[\n",
    "        long_df.groupby(\"asin\")[\"score\"].idxmax(),\n",
    "        [\"asin\", kw_col, sv_col, sales_col, \"sales_per_search\", \"rank\", \"score\"]\n",
    "    ].sort_values(\"score\", ascending=False)\n",
    "\n",
    "    print(\"âœ… Best keyword per ASIN:\")\n",
    "    display(best_per_asin.head(20))\n",
    "\n",
    "    top5_per_asin = (\n",
    "        long_df.sort_values([\"asin\", \"score\"], ascending=[True, False])\n",
    "               .groupby(\"asin\")\n",
    "               .head(5)\n",
    "               .sort_values([\"asin\", \"score\"], ascending=[True, False])\n",
    "               [[ \"asin\", kw_col, sv_col, sales_col, \"sales_per_search\", \"rank\", \"score\" ]]\n",
    "    )\n",
    "    print(\"\\nâœ… Top 5 keywords per ASIN:\")\n",
    "    display(top5_per_asin.head(50))\n",
    "\n",
    "    # Save CSV outputs\n",
    "    out_best = file_path.rsplit(\".\", 1)[0] + \"_best_keyword_per_asin.csv\"\n",
    "    out_top5 = file_path.rsplit(\".\", 1)[0] + \"_top5_keywords_per_asin.csv\"\n",
    "    best_per_asin.to_csv(out_best, index=False)\n",
    "    top5_per_asin.to_csv(out_top5, index=False)\n",
    "    print(f\"\\nðŸ’¾ Saved results to:\\n - {out_best}\\n - {out_top5}\")\n",
    "\n",
    "    # Plots\n",
    "    charts_dir = \"charts\"\n",
    "    os.makedirs(charts_dir, exist_ok=True)\n",
    "\n",
    "    # Use top5_keywords from previous cell if present\n",
    "    if \"top5_keywords\" in globals():\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plt.barh(top5_keywords[kw_col][::-1], top5_keywords[\"score\"][::-1])\n",
    "        plt.title(\"Top 5 Global Keywords by Score\")\n",
    "        plt.xlabel(\"Score\")\n",
    "        plt.ylabel(\"Keyword\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(charts_dir, \"top5_global_keywords.png\"), dpi=150)\n",
    "        plt.show()\n",
    "\n",
    "    # Plot for each ASIN (top 5), limit first 10 to avoid too many figures\n",
    "    for asin in top5_per_asin[\"asin\"].unique()[:10]:\n",
    "        subset = top5_per_asin[top5_per_asin[\"asin\"] == asin].copy()\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plt.barh(subset[kw_col][::-1], subset[\"score\"][::-1])\n",
    "        plt.title(f\"Top 5 Keywords for ASIN {asin.upper()}\")\n",
    "        plt.xlabel(\"Score\")\n",
    "        plt.ylabel(\"Keyword\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(charts_dir, f\"top5_{asin}.png\"), dpi=150)\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"ASIN-based mapping skipped (required columns not found).\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
